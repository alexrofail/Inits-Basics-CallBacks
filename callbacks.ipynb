{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "callbacks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFmQOjInyMU4GgLC03znIz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexrofail/Loss-Optimizers-Training-Loops/blob/main/callbacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifi7vNyMAGc3"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import gzip\n",
        "import pickle\n",
        "from torch import tensor\n",
        "from torch import nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVZFyKo4BAVd"
      },
      "source": [
        "import pickle, gzip, math, torch, matplotlib as mpl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8UzSoYfBC6_"
      },
      "source": [
        "def accuracy(out, yb):\n",
        "  return (torch.argmax(out, dim=1)==yb).float().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XCxng-VBH7g"
      },
      "source": [
        "from torch import optim\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYXSWlw2BNvO"
      },
      "source": [
        "class Dataset():\n",
        "  def __init__(self,x,y):\n",
        "    self.x ,self.y = x,y\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    return self.x[i], self.y[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NR-sGU6BP8k"
      },
      "source": [
        "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
        "  return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
        "          DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw3A6VwBdQB9"
      },
      "source": [
        "def get_data():\n",
        "    with gzip.open('mnist.pkl.gz') as f:\n",
        "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
        "    return map(tensor, (x_train,y_train,x_valid,y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7BsmhrudRSX"
      },
      "source": [
        "x_train,y_train,x_valid,y_valid = get_data()\n",
        "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
        "nh,bs = 50,64\n",
        "c = y_train.max().item()+1\n",
        "loss_func = F.cross_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcGHRBo3Bcop"
      },
      "source": [
        "#TODO: very important, go back and fix the exports thing \n",
        "#TODO: cont... so that you don't have to c/p code above to maintain library"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jd5RFlwBolC"
      },
      "source": [
        "#export\n",
        "class Databunch():\n",
        "  def __init__(self, train_dl, valid_dl, c=None):\n",
        "    self.train_dl, self.valid_dl, self.c = train_dl, valid_dl, c\n",
        "\n",
        "  @property\n",
        "  def train_ds(self): return self.train_dl.dataset\n",
        "\n",
        "  @property\n",
        "  def valid_ds(self): return self.valid_dl.dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85CMpetQf0Eb"
      },
      "source": [
        "data = Databunch(*get_dls(train_ds, valid_ds, bs), c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uolx05qyiJaZ"
      },
      "source": [
        "#export\n",
        "def get_model (data, lr=0.5, nh=50):\n",
        "  m = data.train_ds.x.shape[1]\n",
        "  model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, data.c))\n",
        "  return model, optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "\n",
        "class Learner():\n",
        "  def __init__(self, model, opt, loss_func, data):\n",
        "    self.model, self.opt, self.loss_func, self.data = model, opt, loss_func, data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ7ACOv8m62O"
      },
      "source": [
        "learn = Learner(*get_model(data), loss_func, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hYHJsJ8nJ_K"
      },
      "source": [
        "def fit(epochs, learn):\n",
        "  for epoch in range(epochs):\n",
        "    #training loop\n",
        "    learn.model.train()\n",
        "    for xb, yb in learn.data.train_dl:\n",
        "      loss = learn.loss_func(learn.model(xb), yb)\n",
        "      loss.backward()\n",
        "      learn.opt.step()\n",
        "      learn.opt.zero_grad()\n",
        "    #validation\n",
        "    learn.model.eval\n",
        "    with torch.no_grad():\n",
        "      tot_loss, tot_acc = 0., 0.\n",
        "      for xb, yb in learn.data.valid_dl:\n",
        "        pred = learn.model(xb)\n",
        "        tot_loss += learn.loss_func(pred, yb)\n",
        "        tot_acc += accuracy(pred, yb)\n",
        "\n",
        "      num_vars = len(learn.data.valid_dl)\n",
        "      print(epoch, tot_loss/num_vars, tot_acc/num_vars)\n",
        "\n",
        "    return tot_loss/num_vars, tot_acc/num_vars "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwbenuTYs_WG",
        "outputId": "2e67ab76-365c-4bd3-b993-a1d7adbf4baa"
      },
      "source": [
        "loss, acc = fit(1, learn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.3026) tensor(0.9096)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji6fM9AdtEQi"
      },
      "source": [
        "#Now that we have out fit, we can add callbacks to make it more flexible for an array of 'ifs' without adding 100000 if-else statements\n",
        "def one_batch(xb,yb,cb):\n",
        "  if not cb.begin_batch(xb,yb): return\n",
        "  loss = cb.learn.loss_func(cb.learn.model(xb),yb)\n",
        "  if not cb.after_loss(loss): return\n",
        "  loss.backward()\n",
        "  if cb.after_backward(): cb.learn.opt.step()\n",
        "  if cb.after_step(): cb.learn.opt.zero_grad()\n",
        "\n",
        "def all_batches(dl, cb):\n",
        "  for xb,yb in dl:\n",
        "    one_batch(xb,yb,cb)\n",
        "    if cb.do_stop(): return \n",
        "\n",
        "def fit(epochs, learn, cb):\n",
        "  if not cb.begin_fit(learn): return\n",
        "  for epoch in range (epochs):\n",
        "    if not cb.begin_epoch(epoch): return\n",
        "    all_batches(learn.data.train_dl, cb)\n",
        "\n",
        "    if cb.begin_validate():\n",
        "      with torch.no_grad(): all_batches(learn.data.valid_dl,cb)\n",
        "    if cb.do_stop() or not cb.after_epoch(): break\n",
        "\n",
        "  cb.after_fit()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lj_6SRxxbQ0"
      },
      "source": [
        "class Callback():\n",
        "  def begin_fit(self, learn):\n",
        "          self.learn = learn\n",
        "          return True\n",
        "  \n",
        "  def after_fit(self): return True\n",
        "  \n",
        "  def begin_epoch(self, epoch):\n",
        "      self.epoch=epoch\n",
        "      return True\n",
        " \n",
        "  def begin_validate(self): return True\n",
        " \n",
        "  def after_epoch(self): return True\n",
        " \n",
        "  def begin_batch(self, xb, yb):\n",
        "      self.xb,self.yb = xb,yb\n",
        "      return True\n",
        " \n",
        "  def after_loss(self, loss):\n",
        "      self.loss = loss\n",
        "      return True\n",
        " \n",
        "  def after_backward(self): return True\n",
        " \n",
        "  def after_step(self): return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXQXLI4OykTG"
      },
      "source": [
        "#Basically Fast.ai's cb handler right now\n",
        "class CallbackHandler():\n",
        "  def __init__(self, cbs=None):\n",
        "    self.cbs = cbs if cbs else []\n",
        "\n",
        "  def begin_fit(self, learn):\n",
        "    self.learn, self.in_train = learn, True\n",
        "    learn.stop = False\n",
        "    res = True\n",
        "    for cb in self.cbs:\n",
        "      res = res and cb.begin_fit(learn)\n",
        "    return res\n",
        "\n",
        "  def after_fit(self):\n",
        "    res = not self.in_train\n",
        "    for cb in self.cbs: res = res and cb.after_fit()\n",
        "    return res\n",
        "\n",
        "  def begin_epoch(self, epoch):\n",
        "    self.learn.model.train()\n",
        "    self.in_train=True\n",
        "    res = True\n",
        "    for cb in self.cbs: res = res and cb.begin_epoch(epoch)\n",
        "    return res \n",
        "\n",
        "  def begin_validate(self):\n",
        "    self.learn.model.eval()\n",
        "    self.in_train=False\n",
        "    res = True\n",
        "    for cb in self.cbs: res = res and cb.begin_validate()\n",
        "    return res \n",
        "\n",
        "  def after_epoch(self):\n",
        "    res = True\n",
        "    for cb in self.cbs: res = res and cb.after_epoch()\n",
        "    return res \n",
        "\n",
        "  def begin_batch(self, xb,yb):\n",
        "    res = True\n",
        "    for cb in self.cbs: res = res and cb.begin_batch(xb,yb)\n",
        "    return res \n",
        "\n",
        "  def after_loss(self, loss):\n",
        "    res = True\n",
        "    for cb in self.cbs: res = res and cb.after_loss(loss)\n",
        "    return res \n",
        "\n",
        "  def after_backward(self):\n",
        "    res = True\n",
        "    for cb in self.cbs: res = res and cb.after_backward()\n",
        "    return res \n",
        "\n",
        "  def after_step(self):\n",
        "    res = True\n",
        "    for cb in self.cbs: res = res and cb.after_step()\n",
        "    return res \n",
        "\n",
        "  def do_stop(self):\n",
        "    try: return self.learn.stop\n",
        "    finally: self.learn.stop = False\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJr7pnPwrfTL"
      },
      "source": [
        "#Testing callbacks\n",
        "class TestCallBack(Callback):\n",
        "  def begin_fit(self,learn):\n",
        "    super().begin_fit(learn)\n",
        "    self.n_iters = 0\n",
        "    return True \n",
        "\n",
        "  def after_step(self):\n",
        "    self.n_iters += 1\n",
        "    print(self.n_iters)\n",
        "    if self.n_iters >=10: self.learn.stop = True\n",
        "    return True "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "5pWYaWFswYk2",
        "outputId": "1f81549b-6c8f-4e74-ba83-28b05ada781a"
      },
      "source": [
        "fit(1, learn, cb=CallbackHandler([TestCallBack()]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-03aa68bc9229>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCallbackHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTestCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-973435e7c2cd>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, cb)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m       \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-973435e7c2cd>\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(dl, cb)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-973435e7c2cd>\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(xb, yb, cb)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKrZ7IT7wiBA"
      },
      "source": [
        "#export\n",
        "import re\n",
        "\n",
        "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
        "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
        "def camel2snake(name):\n",
        "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
        "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n",
        "\n",
        "class Callback():\n",
        "    _order=0\n",
        "    def set_runner(self, run): self.run=run\n",
        "    def __getattr__(self, k): return getattr(self.run, k)\n",
        "    @property\n",
        "    def name(self):\n",
        "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
        "        return camel2snake(name or 'callback')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NmuBtsHxKNT"
      },
      "source": [
        "#export\n",
        "#Callback to switch model between training and validation mode\n",
        "class TrainEvalCallBack(Callback):\n",
        "  def begin_fit(self):\n",
        "    self.run.n_epochs=0.\n",
        "    self.run.n_iter=0\n",
        "\n",
        "  def after_batch(self):\n",
        "    if not self.in_train: return\n",
        "    self.run.n_epochs += 1./self.iters\n",
        "    self.run.n_iter += 1\n",
        "\n",
        "  def begin_epoch(self):\n",
        "    self.run.n_epochs = self.epoch \n",
        "    self.model.train()\n",
        "    self.run.in_train=True\n",
        "\n",
        "  def begin_validate(self):\n",
        "    self.model.eval()\n",
        "    self.run.in_train=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs8HjaWjyXC4"
      },
      "source": [
        "class TestCallback(Callback):\n",
        "    def after_step(self):\n",
        "        if self.train_eval.n_iters>=10: return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7kmHeWCEyggF",
        "outputId": "115af267-a688-4041-94b0-a9bb4626a665"
      },
      "source": [
        "cbname= 'TrainEvalCallback'\n",
        "camel2snake(cbname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'train_eval_callback'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2QlxKx4ymC2"
      },
      "source": [
        "#export\n",
        "from typing import *\n",
        "\n",
        "def listify(o):\n",
        "    if o is None: return []\n",
        "    if isinstance(o, list): return o\n",
        "    if isinstance(o, str): return [o]\n",
        "    if isinstance(o, Iterable): return list(o)\n",
        "    return [o]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5dDtpQJAC_P"
      },
      "source": [
        "#export\n",
        "class Runner():\n",
        "    def __init__(self, cbs=None, cb_funcs=None):\n",
        "        cbs = listify(cbs)\n",
        "        for cbf in listify(cb_funcs):\n",
        "            cb = cbf()\n",
        "            setattr(self, cb.name, cb)\n",
        "            cbs.append(cb)\n",
        "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
        "\n",
        "    @property\n",
        "    def opt(self):       return self.learn.opt\n",
        "    @property\n",
        "    def model(self):     return self.learn.model\n",
        "    @property\n",
        "    def loss_func(self): return self.learn.loss_func\n",
        "    @property\n",
        "    def data(self):      return self.learn.data\n",
        "\n",
        "    def one_batch(self, xb, yb):\n",
        "        self.xb,self.yb = xb,yb\n",
        "        if self('begin_batch'): return\n",
        "        self.pred = self.model(self.xb)\n",
        "        if self('after_pred'): return\n",
        "        self.loss = self.loss_func(self.pred, self.yb)\n",
        "        if self('after_loss') or not self.in_train: return\n",
        "        self.loss.backward()\n",
        "        if self('after_backward'): return\n",
        "        self.opt.step()\n",
        "        if self('after_step'): return\n",
        "        self.opt.zero_grad()\n",
        "\n",
        "    def all_batches(self, dl):\n",
        "        self.iters = len(dl)\n",
        "        for xb,yb in dl:\n",
        "            if self.stop: break\n",
        "            self.one_batch(xb, yb)\n",
        "            self('after_batch')\n",
        "        self.stop=False\n",
        "\n",
        "    def fit(self, epochs, learn):\n",
        "        self.epochs,self.learn = epochs,learn\n",
        "\n",
        "        try:\n",
        "            for cb in self.cbs: cb.set_runner(self)\n",
        "            if self('begin_fit'): return\n",
        "            for epoch in range(epochs):\n",
        "                self.epoch = epoch\n",
        "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
        "\n",
        "                with torch.no_grad(): \n",
        "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
        "                if self('after_epoch'): break\n",
        "            \n",
        "        finally:\n",
        "            self('after_fit')\n",
        "            self.learn = None\n",
        "\n",
        "    def __call__(self, cb_name):\n",
        "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
        "            f = getattr(cb, cb_name, None)\n",
        "            if f and f(): return True\n",
        "        return False"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M7uN2m8AHCB"
      },
      "source": [
        "#export\n",
        "class AvgStats():\n",
        "    def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train\n",
        "    \n",
        "    def reset(self):\n",
        "        self.tot_loss,self.count = 0.,0\n",
        "        self.tot_mets = [0.] * len(self.metrics)\n",
        "        \n",
        "    @property\n",
        "    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
        "    @property\n",
        "    def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        if not self.count: return \"\"\n",
        "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
        "\n",
        "    def accumulate(self, run):\n",
        "        bn = run.xb.shape[0]\n",
        "        self.tot_loss += run.loss * bn\n",
        "        self.count += bn\n",
        "        for i,m in enumerate(self.metrics):\n",
        "            self.tot_mets[i] += m(run.pred, run.yb) * bn\n",
        "\n",
        "class AvgStatsCallback(Callback):\n",
        "    def __init__(self, metrics):\n",
        "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
        "        \n",
        "    def begin_epoch(self):\n",
        "        self.train_stats.reset()\n",
        "        self.valid_stats.reset()\n",
        "        \n",
        "    def after_loss(self):\n",
        "        stats = self.train_stats if self.in_train else self.valid_stats\n",
        "        with torch.no_grad(): stats.accumulate(self.run)\n",
        "    \n",
        "    def after_epoch(self):\n",
        "        print(self.train_stats)\n",
        "        print(self.valid_stats)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94DPHM1RAPAy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}